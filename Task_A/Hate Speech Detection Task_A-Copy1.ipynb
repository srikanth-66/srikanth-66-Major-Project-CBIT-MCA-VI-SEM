{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Based Fusion Approach for Hate Speech Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import copy\n",
    "import re\n",
    "import csv\n",
    "from keras.utils import to_categorical\n",
    "import codecs\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dense, Dropout, Activation , LSTM , Input , Embedding\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Input, Dense, concatenate, Activation, Average\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout , Bidirectional\n",
    "from keras.layers import Flatten , LSTM , Reshape, LeakyReLU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
    "from keras.regularizers import L1L2\n",
    "from keras import optimizers\n",
    "from keras.callbacks import CSVLogger\n",
    "import keras\n",
    "import keras.preprocessing.text as kpt\n",
    "#from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\M Srikant\\\\Desktop\\\\160118862036_DL_HPD\\\\CODE\\\\Original Code\\\\Task_A\\\\Data'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"C:/Users/M Srikant/Desktop/160118862036_DL_HPD/CODE/Original Code/Task_A/Data\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'text', 'HS', 'TR', 'AG']\n"
     ]
    }
   ],
   "source": [
    "#train.tsv\n",
    "train=pd.read_table('train_en.tsv',delimiter='\\t',encoding='utf-8')\n",
    "print(list(train.columns.values)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>HS</th>\n",
       "      <th>TR</th>\n",
       "      <th>AG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201</td>\n",
       "      <td>Hurray, saving us $$$ in so many ways @potus @...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202</td>\n",
       "      <td>Why would young fighting age men be the vast m...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>203</td>\n",
       "      <td>@KamalaHarris Illegals Dump their Kids at the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>204</td>\n",
       "      <td>NY Times: 'Nearly All White' States Pose 'an A...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>205</td>\n",
       "      <td>Orban in Brussels: European leaders are ignori...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                               text  HS  TR  AG\n",
       "0  201  Hurray, saving us $$$ in so many ways @potus @...   1   0   0\n",
       "1  202  Why would young fighting age men be the vast m...   1   0   0\n",
       "2  203  @KamalaHarris Illegals Dump their Kids at the ...   1   0   0\n",
       "3  204  NY Times: 'Nearly All White' States Pose 'an A...   0   0   0\n",
       "4  205  Orban in Brussels: European leaders are ignori...   0   0   0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id      9000\n",
       "text    9000\n",
       "HS      9000\n",
       "TR      9000\n",
       "AG      9000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Hurray, saving us $$$ in so many ways @potus @...\n",
      "1    Why would young fighting age men be the vast m...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(my_df.text.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_word_stop = ['the','in','of','is','a','to','an','be','are','for','was','it','as','on', 'so', 'at']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('segmentation_train_dev.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for row in my_df.text:\n",
    "        row = re.sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\",\"url\",row)\n",
    "        row = \" \".join([word for word in row.split() if word not in my_word_stop])\n",
    "        row = re.sub(r\"(.)\\1+\", r\"\\1\",row)\n",
    "        rest_array = [text.encode(\"utf8\") for text in row]\n",
    "        rest_array = b\"\".join(rest_array)\n",
    "        writer.writerow(rest_array)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_table('segmentation_train_dev.csv',delimiter='\\t',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    9000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[0] = train[0].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = my_df.HS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df.HS.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = Tokenizer(lower=False,filters=',',char_level=False)\n",
    "tokenizer.fit_on_texts(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M Srikant\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dictionary = tokenizer.word_index\n",
    "with open('dictionary.p', 'wb') as fp:\n",
    "    pickle.dump(dictionary, fp)\n",
    "\n",
    "\n",
    "def convert_text_to_index_array(text):\n",
    "    temp_wordIndices = []\n",
    "    for word in kpt.text_to_word_sequence(text,filters=',',lower=False):\n",
    "        if word in dictionary:\n",
    "            temp_wordIndices.append(dictionary[word])\n",
    "    return temp_wordIndices\n",
    "\n",
    "allWordIndices = []\n",
    "for text in train_x:\n",
    "    wordIndices = convert_text_to_index_array(text)\n",
    "    allWordIndices.append(wordIndices)\n",
    "\n",
    "allWordIndices = np.asarray(allWordIndices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 170 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "print('Found %s unique tokens.' % len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = tokenizer.sequences_to_matrix(allWordIndices, mode='binary')\n",
    "train_y = to_categorical(train_y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.advanced_activations import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               17200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 37,802\n",
      "Trainable params: 37,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 7200 samples, validate on 1800 samples\n",
      "Epoch 1/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.6379 - accuracy: 0.6317 - val_loss: 0.8754 - val_accuracy: 0.3861\n",
      "Epoch 2/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.5883 - accuracy: 0.6822 - val_loss: 0.7476 - val_accuracy: 0.4928\n",
      "Epoch 3/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.5703 - accuracy: 0.6994 - val_loss: 0.8013 - val_accuracy: 0.4944\n",
      "Epoch 4/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.5544 - accuracy: 0.7106 - val_loss: 0.8272 - val_accuracy: 0.5072\n",
      "Epoch 5/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.5406 - accuracy: 0.7200 - val_loss: 0.8312 - val_accuracy: 0.4878\n",
      "Epoch 6/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.5282 - accuracy: 0.7286 - val_loss: 0.9098 - val_accuracy: 0.4989\n",
      "Epoch 7/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.5170 - accuracy: 0.7332 - val_loss: 0.7418 - val_accuracy: 0.6072\n",
      "Epoch 8/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.5034 - accuracy: 0.7419 - val_loss: 0.8256 - val_accuracy: 0.5228\n",
      "Epoch 9/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.4836 - accuracy: 0.7579 - val_loss: 0.8917 - val_accuracy: 0.5117\n",
      "Epoch 10/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.4715 - accuracy: 0.7638 - val_loss: 0.9516 - val_accuracy: 0.4894\n",
      "Epoch 11/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.4614 - accuracy: 0.7647 - val_loss: 1.0030 - val_accuracy: 0.4950\n",
      "Epoch 12/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.4470 - accuracy: 0.7753 - val_loss: 1.0563 - val_accuracy: 0.5000\n",
      "Epoch 13/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.4469 - accuracy: 0.7763 - val_loss: 0.8105 - val_accuracy: 0.5639\n",
      "Epoch 14/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.4236 - accuracy: 0.7885 - val_loss: 1.1355 - val_accuracy: 0.4611\n",
      "Epoch 15/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.4170 - accuracy: 0.7893 - val_loss: 1.0219 - val_accuracy: 0.5078\n",
      "Epoch 16/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.4022 - accuracy: 0.7957 - val_loss: 1.0874 - val_accuracy: 0.5361\n",
      "Epoch 17/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.4066 - accuracy: 0.7967 - val_loss: 1.3857 - val_accuracy: 0.4350\n",
      "Epoch 18/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.3863 - accuracy: 0.8094 - val_loss: 1.4324 - val_accuracy: 0.4244\n",
      "Epoch 19/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.3775 - accuracy: 0.8104 - val_loss: 1.3599 - val_accuracy: 0.4450\n",
      "Epoch 20/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.3722 - accuracy: 0.8165 - val_loss: 1.4269 - val_accuracy: 0.4539\n",
      "Epoch 21/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.3664 - accuracy: 0.8183 - val_loss: 0.9605 - val_accuracy: 0.5633\n",
      "Epoch 22/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.3569 - accuracy: 0.8257 - val_loss: 1.5287 - val_accuracy: 0.4806\n",
      "Epoch 23/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.3490 - accuracy: 0.8293 - val_loss: 1.4204 - val_accuracy: 0.5056\n",
      "Epoch 24/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.3444 - accuracy: 0.8292 - val_loss: 1.3696 - val_accuracy: 0.5183\n",
      "Epoch 25/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.3318 - accuracy: 0.8440 - val_loss: 1.3500 - val_accuracy: 0.4872\n",
      "Epoch 26/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.3281 - accuracy: 0.8414 - val_loss: 1.5795 - val_accuracy: 0.5361\n",
      "Epoch 27/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.3302 - accuracy: 0.8403 - val_loss: 1.5476 - val_accuracy: 0.4967\n",
      "Epoch 28/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.3273 - accuracy: 0.8457 - val_loss: 1.5840 - val_accuracy: 0.4517\n",
      "Epoch 29/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.3222 - accuracy: 0.8494 - val_loss: 1.3642 - val_accuracy: 0.5211\n",
      "Epoch 30/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.3126 - accuracy: 0.8526 - val_loss: 1.8494 - val_accuracy: 0.5106\n",
      "Epoch 31/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.3165 - accuracy: 0.8550 - val_loss: 1.6859 - val_accuracy: 0.5156\n",
      "Epoch 32/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.2991 - accuracy: 0.8617 - val_loss: 1.6943 - val_accuracy: 0.4978\n",
      "Epoch 33/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.3029 - accuracy: 0.8626 - val_loss: 1.7529 - val_accuracy: 0.4911\n",
      "Epoch 34/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.2976 - accuracy: 0.8610 - val_loss: 1.6338 - val_accuracy: 0.5000\n",
      "Epoch 35/100\n",
      "7200/7200 [==============================] - 10s 1ms/step - loss: 0.2958 - accuracy: 0.8662 - val_loss: 1.5943 - val_accuracy: 0.4994\n",
      "Epoch 36/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.2783 - accuracy: 0.8662 - val_loss: 2.0435 - val_accuracy: 0.4706\n",
      "Epoch 37/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.2859 - accuracy: 0.8674 - val_loss: 1.8203 - val_accuracy: 0.5367\n",
      "Epoch 38/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.2855 - accuracy: 0.8681 - val_loss: 1.4158 - val_accuracy: 0.5711\n",
      "Epoch 39/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.2794 - accuracy: 0.8719 - val_loss: 1.8580 - val_accuracy: 0.5250\n",
      "Epoch 40/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.2784 - accuracy: 0.8651 - val_loss: 1.5772 - val_accuracy: 0.5422\n",
      "Epoch 41/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.2731 - accuracy: 0.8737 - val_loss: 1.6824 - val_accuracy: 0.5144\n",
      "Epoch 42/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.2749 - accuracy: 0.8726 - val_loss: 2.0657 - val_accuracy: 0.4267\n",
      "Epoch 43/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.2639 - accuracy: 0.8796 - val_loss: 1.6247 - val_accuracy: 0.5611\n",
      "Epoch 44/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.2614 - accuracy: 0.8788 - val_loss: 1.8807 - val_accuracy: 0.5261\n",
      "Epoch 45/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.2592 - accuracy: 0.8838 - val_loss: 1.9361 - val_accuracy: 0.4817\n",
      "Epoch 46/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.2668 - accuracy: 0.8786 - val_loss: 1.6207 - val_accuracy: 0.5472\n",
      "Epoch 47/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.2643 - accuracy: 0.8807 - val_loss: 2.0097 - val_accuracy: 0.5017\n",
      "Epoch 48/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.2591 - accuracy: 0.8794 - val_loss: 1.9880 - val_accuracy: 0.4767\n",
      "Epoch 49/100\n",
      "7200/7200 [==============================] - 10s 1ms/step - loss: 0.2525 - accuracy: 0.8860 - val_loss: 2.0109 - val_accuracy: 0.4911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.2540 - accuracy: 0.8826 - val_loss: 2.0940 - val_accuracy: 0.4839\n",
      "Epoch 51/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.2506 - accuracy: 0.8849 - val_loss: 1.9955 - val_accuracy: 0.5033\n",
      "Epoch 52/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.2540 - accuracy: 0.8849 - val_loss: 1.6633 - val_accuracy: 0.5322\n",
      "Epoch 53/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.2507 - accuracy: 0.8864 - val_loss: 2.4789 - val_accuracy: 0.4672\n",
      "Epoch 54/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.2471 - accuracy: 0.8893 - val_loss: 2.0309 - val_accuracy: 0.4906\n",
      "Epoch 55/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.2428 - accuracy: 0.8893 - val_loss: 1.9056 - val_accuracy: 0.5106\n",
      "Epoch 56/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.2368 - accuracy: 0.8974 - val_loss: 2.0769 - val_accuracy: 0.4983\n",
      "Epoch 57/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.2375 - accuracy: 0.8950 - val_loss: 2.0077 - val_accuracy: 0.5256\n",
      "Epoch 58/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.2277 - accuracy: 0.8964 - val_loss: 2.5384 - val_accuracy: 0.4789\n",
      "Epoch 59/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.2383 - accuracy: 0.8917 - val_loss: 2.0126 - val_accuracy: 0.5056\n",
      "Epoch 60/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.2335 - accuracy: 0.8954 - val_loss: 2.2479 - val_accuracy: 0.4928\n",
      "Epoch 61/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.2327 - accuracy: 0.8958 - val_loss: 2.3400 - val_accuracy: 0.4817\n",
      "Epoch 62/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.2224 - accuracy: 0.9033 - val_loss: 2.3593 - val_accuracy: 0.4961\n",
      "Epoch 63/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.2224 - accuracy: 0.9025 - val_loss: 2.4764 - val_accuracy: 0.4761\n",
      "Epoch 64/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.2227 - accuracy: 0.9000 - val_loss: 2.0829 - val_accuracy: 0.5072\n",
      "Epoch 65/100\n",
      "7200/7200 [==============================] - 7s 1ms/step - loss: 0.2214 - accuracy: 0.9021 - val_loss: 2.2377 - val_accuracy: 0.5089\n",
      "Epoch 66/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.2218 - accuracy: 0.8990 - val_loss: 2.0978 - val_accuracy: 0.5133\n",
      "Epoch 67/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.2232 - accuracy: 0.9001 - val_loss: 2.4255 - val_accuracy: 0.4722\n",
      "Epoch 68/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.2221 - accuracy: 0.9046 - val_loss: 2.0427 - val_accuracy: 0.5217\n",
      "Epoch 69/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.2138 - accuracy: 0.9083 - val_loss: 2.3664 - val_accuracy: 0.4661\n",
      "Epoch 70/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.2250 - accuracy: 0.8976 - val_loss: 1.9939 - val_accuracy: 0.5467\n",
      "Epoch 71/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.2181 - accuracy: 0.9057 - val_loss: 2.2430 - val_accuracy: 0.5339\n",
      "Epoch 72/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.2042 - accuracy: 0.9140 - val_loss: 2.1013 - val_accuracy: 0.5189\n",
      "Epoch 73/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.2081 - accuracy: 0.9097 - val_loss: 1.8969 - val_accuracy: 0.5494\n",
      "Epoch 74/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.2161 - accuracy: 0.9064 - val_loss: 2.0333 - val_accuracy: 0.5283\n",
      "Epoch 75/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.2022 - accuracy: 0.9161 - val_loss: 2.4446 - val_accuracy: 0.5267\n",
      "Epoch 76/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.2091 - accuracy: 0.9081 - val_loss: 2.7634 - val_accuracy: 0.4794\n",
      "Epoch 77/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.2131 - accuracy: 0.9093 - val_loss: 2.7550 - val_accuracy: 0.4928\n",
      "Epoch 78/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.2078 - accuracy: 0.9103 - val_loss: 2.6156 - val_accuracy: 0.4744\n",
      "Epoch 79/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.2062 - accuracy: 0.9110 - val_loss: 2.3464 - val_accuracy: 0.5106\n",
      "Epoch 80/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.2018 - accuracy: 0.9144 - val_loss: 2.3330 - val_accuracy: 0.5022\n",
      "Epoch 81/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.1977 - accuracy: 0.9158 - val_loss: 2.3324 - val_accuracy: 0.4961\n",
      "Epoch 82/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.2110 - accuracy: 0.9094 - val_loss: 2.1245 - val_accuracy: 0.5078\n",
      "Epoch 83/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.2154 - accuracy: 0.9115 - val_loss: 2.3158 - val_accuracy: 0.4972\n",
      "Epoch 84/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.2005 - accuracy: 0.9151 - val_loss: 2.5242 - val_accuracy: 0.5006\n",
      "Epoch 85/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.1975 - accuracy: 0.9171 - val_loss: 2.5742 - val_accuracy: 0.4967\n",
      "Epoch 86/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.2011 - accuracy: 0.9128 - val_loss: 2.8147 - val_accuracy: 0.5072\n",
      "Epoch 87/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.1912 - accuracy: 0.9182 - val_loss: 2.4617 - val_accuracy: 0.5072\n",
      "Epoch 88/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.1994 - accuracy: 0.9164 - val_loss: 2.2167 - val_accuracy: 0.5083\n",
      "Epoch 89/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.1937 - accuracy: 0.9158 - val_loss: 2.6293 - val_accuracy: 0.4844\n",
      "Epoch 90/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.1897 - accuracy: 0.9186 - val_loss: 2.4959 - val_accuracy: 0.4850\n",
      "Epoch 91/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.1889 - accuracy: 0.9197 - val_loss: 2.2780 - val_accuracy: 0.5056\n",
      "Epoch 92/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.1903 - accuracy: 0.9165 - val_loss: 2.3583 - val_accuracy: 0.5328\n",
      "Epoch 93/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.1904 - accuracy: 0.9228 - val_loss: 2.9282 - val_accuracy: 0.4867\n",
      "Epoch 94/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.1998 - accuracy: 0.9158 - val_loss: 2.6020 - val_accuracy: 0.5161\n",
      "Epoch 95/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.1850 - accuracy: 0.9197 - val_loss: 2.8797 - val_accuracy: 0.4722\n",
      "Epoch 96/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.1852 - accuracy: 0.9218 - val_loss: 2.6170 - val_accuracy: 0.5361\n",
      "Epoch 97/100\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.1861 - accuracy: 0.9242 - val_loss: 2.8109 - val_accuracy: 0.5017\n",
      "Epoch 98/100\n",
      "7200/7200 [==============================] - 11s 2ms/step - loss: 0.1923 - accuracy: 0.9235 - val_loss: 2.3265 - val_accuracy: 0.5233\n",
      "Epoch 99/100\n",
      "7200/7200 [==============================] - 10s 1ms/step - loss: 0.1969 - accuracy: 0.9186 - val_loss: 2.8356 - val_accuracy: 0.4728\n",
      "Epoch 100/100\n",
      "7200/7200 [==============================] - 11s 1ms/step - loss: 0.1810 - accuracy: 0.9228 - val_loss: 2.7190 - val_accuracy: 0.4944\n",
      "saved model!\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(100, activation='relu',input_shape=(train_x.shape[1],)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "filepath=\"sequencing_the_data_try_n_error.{epoch:02d}-{val_loss:.4f}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "csv_logger = CSVLogger('final_log.csv', append=True, separator=';')\n",
    "\n",
    "model.fit(train_x, train_y,\n",
    "    batch_size=5,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    validation_split=0.20,\n",
    "    shuffle=True,callbacks = [csv_logger])\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open('model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights('model.h5')\n",
    "\n",
    "print('saved model!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'text', 'HS', 'TR', 'AG']\n"
     ]
    }
   ],
   "source": [
    "#test.tsv\n",
    "test=pd.read_table('dev_en.tsv',delimiter='\\t',encoding='utf-8')\n",
    "print(list(test.columns.values)) #file header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>HS</th>\n",
       "      <th>TR</th>\n",
       "      <th>AG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18201</td>\n",
       "      <td>I swear I’m getting to places just in the nick...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18202</td>\n",
       "      <td>I’m an immigrant — and Trump is right on immig...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18203</td>\n",
       "      <td>#IllegalImmigrants #IllegalAliens #ElectoralSy...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18204</td>\n",
       "      <td>@DRUDGE_REPORT We have our own invasion issues...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18205</td>\n",
       "      <td>Worker Charged With Sexually Molesting Eight C...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text  HS  TR  AG\n",
       "0  18201  I swear I’m getting to places just in the nick...   0   0   0\n",
       "1  18202  I’m an immigrant — and Trump is right on immig...   0   0   0\n",
       "2  18203  #IllegalImmigrants #IllegalAliens #ElectoralSy...   1   0   1\n",
       "3  18204  @DRUDGE_REPORT We have our own invasion issues...   1   0   1\n",
       "4  18205  Worker Charged With Sexually Molesting Eight C...   0   0   0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('segmentation_test.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for row in test.text:\n",
    "        row = re.sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\",\"url\",row)\n",
    "        row = \" \".join([word for word in row.split() if word not in my_word_stop])\n",
    "        row = re.sub(r\"(.)\\1+\", r\"\\1\",row)\n",
    "        rest_array = [text.encode(\"utf8\") for text in row]\n",
    "        rest_array = b\"\".join(rest_array)\n",
    "        writer.writerow(rest_array)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "test_final=pd.read_csv('segmentation_test.csv',delimiter='\\t',header=None)\n",
    "print(list(test_final.columns.values)) #file header\n",
    "test_final[0] = test_final[0].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_array[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "word not found :  0\n",
      "1000\n",
      "word found :  [36, 2, 3, 7, 10, 17, 1, 2, 24, 2, 7, 17, 1, 19, 8, 14, 3, 6, 1, 26, 1, 22, 6, 8, 19, 1, 12, 3, 9, 1, 28, 14, 2, 25, 8, 1, 4, 12, 2, 5, 7, 1, 21, 2, 13, 1, 52, 9, 2, 53, 11, 3, 10, 1, 12, 3, 7, 2, 9, 14, 2, 6, 4, 31, 3, 20, 11, 9, 2, 31, 7, 3, 18, 2, 1, 3, 10, 1, 52, 1, 17, 2, 9, 1, 5, 4, 32, 9, 1, 12, 2, 3, 7, 4, 20, 7, 2, 3, 22, 5, 6, 16, 31, 1, 17, 2, 9, 1, 4, 12, 2, 7, 2, 32, 9, 1, 14, 11, 15, 12, 1, 62, 1, 13, 8, 23]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=train_x.shape[1])\n",
    "\n",
    "\n",
    "with open('dictionary.p', 'rb') as fp:\n",
    "    dictionary = pickle.load(fp)\n",
    "    \n",
    "    \n",
    "not_found_word_list = []\n",
    "def convert_text_to_index_array(text):\n",
    "    words = kpt.text_to_word_sequence(text,filters='',lower=False,split=',')\n",
    "    wordIndices = []\n",
    "    no_word = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            wordIndices.append(dictionary[word])\n",
    "        else:\n",
    "            if word == \"\":\n",
    "                not_found_word_list.append(word)\n",
    "                no_word = no_word + 1\n",
    "    \n",
    "    return wordIndices,no_word\n",
    "\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "model.load_weights('model.h5')\n",
    "with open('fileName.csv', 'w') as f:\n",
    "    count=0\n",
    "    no_words = 0\n",
    "    for row in test_final[0]:\n",
    "        evalSentence = row\n",
    "        testArr,no_word = convert_text_to_index_array(evalSentence)\n",
    "        input = tokenizer.sequences_to_matrix([testArr], mode='binary')\n",
    "        pred = model.predict(input)\n",
    "        f.write(str(np.argmax(pred)) + \"\\n\")\n",
    "        count+=1\n",
    "        no_words+=no_word\n",
    "f.close()\n",
    "print(count)\n",
    "print(\"word not found : \", no_words)\n",
    "print(count)\n",
    "print(\"word found : \", wordIndices)\n",
    "with open('not_found_word_list.csv', 'w') as f:\n",
    "    for word in not_found_word_list:\n",
    "        f.write(str(word)+\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
