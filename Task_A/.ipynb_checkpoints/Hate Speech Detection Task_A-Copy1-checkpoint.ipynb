{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Based Fusion Approach for Hate Speech Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import copy\n",
    "import re\n",
    "import csv\n",
    "from keras.utils import to_categorical\n",
    "import codecs\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dense, Dropout, Activation , LSTM , Input , Embedding\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Input, Dense, concatenate, Activation, Average\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout , Bidirectional\n",
    "from keras.layers import Flatten , LSTM , Reshape, LeakyReLU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
    "from keras.regularizers import L1L2\n",
    "from keras import optimizers\n",
    "from keras.callbacks import CSVLogger\n",
    "import keras\n",
    "import keras.preprocessing.text as kpt\n",
    "#from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\M Srikant\\\\Desktop\\\\160118862036_DL_HPD\\\\CODE\\\\Original Code\\\\Task_A\\\\Data'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"C:/Users/M Srikant/Desktop/160118862036_DL_HPD/CODE/Original Code/Task_A/Data\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'text', 'HS', 'TR', 'AG']\n"
     ]
    }
   ],
   "source": [
    "#train.tsv\n",
    "train=pd.read_table('train_en.tsv',delimiter='\\t',encoding='utf-8')\n",
    "print(list(train.columns.values)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>HS</th>\n",
       "      <th>TR</th>\n",
       "      <th>AG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201</td>\n",
       "      <td>Hurray, saving us $$$ in so many ways @potus @...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202</td>\n",
       "      <td>Why would young fighting age men be the vast m...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>203</td>\n",
       "      <td>@KamalaHarris Illegals Dump their Kids at the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>204</td>\n",
       "      <td>NY Times: 'Nearly All White' States Pose 'an A...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>205</td>\n",
       "      <td>Orban in Brussels: European leaders are ignori...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                               text  HS  TR  AG\n",
       "0  201  Hurray, saving us $$$ in so many ways @potus @...   1   0   0\n",
       "1  202  Why would young fighting age men be the vast m...   1   0   0\n",
       "2  203  @KamalaHarris Illegals Dump their Kids at the ...   1   0   0\n",
       "3  204  NY Times: 'Nearly All White' States Pose 'an A...   0   0   0\n",
       "4  205  Orban in Brussels: European leaders are ignori...   0   0   0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id      9000\n",
       "text    9000\n",
       "HS      9000\n",
       "TR      9000\n",
       "AG      9000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Hurray, saving us $$$ in so many ways @potus @...\n",
      "1    Why would young fighting age men be the vast m...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(my_df.text.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_word_stop = ['the','in','of','is','a','to','an','be','are','for','was','it','as','on', 'so', 'at']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('segmentation_train_dev.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for row in my_df.text:\n",
    "        row = re.sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\",\"url\",row)\n",
    "        row = \" \".join([word for word in row.split() if word not in my_word_stop])\n",
    "        row = re.sub(r\"(.)\\1+\", r\"\\1\",row)\n",
    "        rest_array = [text.encode(\"utf8\") for text in row]\n",
    "        rest_array = b\"\".join(rest_array)\n",
    "        writer.writerow(rest_array)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_table('segmentation_train_dev.csv',delimiter='\\t',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    9000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[0] = train[0].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = my_df.HS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df.HS.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = Tokenizer(lower=False,filters=',',char_level=False)\n",
    "tokenizer.fit_on_texts(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M Srikant\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dictionary = tokenizer.word_index\n",
    "with open('dictionary.p', 'wb') as fp:\n",
    "    pickle.dump(dictionary, fp)\n",
    "\n",
    "\n",
    "def convert_text_to_index_array(text):\n",
    "    temp_wordIndices = []\n",
    "    for word in kpt.text_to_word_sequence(text,filters=',',lower=False):\n",
    "        if word in dictionary:\n",
    "            temp_wordIndices.append(dictionary[word])\n",
    "    return temp_wordIndices\n",
    "\n",
    "allWordIndices = []\n",
    "for text in train_x:\n",
    "    wordIndices = convert_text_to_index_array(text)\n",
    "    allWordIndices.append(wordIndices)\n",
    "\n",
    "allWordIndices = np.asarray(allWordIndices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 170 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "print('Found %s unique tokens.' % len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = tokenizer.sequences_to_matrix(allWordIndices, mode='binary')\n",
    "train_y = to_categorical(train_y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.advanced_activations import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               17200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 37,802\n",
      "Trainable params: 37,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 7200 samples, validate on 1800 samples\n",
      "Epoch 1/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.6364 - accuracy: 0.6397 - val_loss: 0.6742 - val_accuracy: 0.5700\n",
      "Epoch 2/100\n",
      "7200/7200 [==============================] - 4s 575us/step - loss: 0.5886 - accuracy: 0.6810 - val_loss: 0.8406 - val_accuracy: 0.4206\n",
      "Epoch 3/100\n",
      "7200/7200 [==============================] - 4s 521us/step - loss: 0.5736 - accuracy: 0.7025 - val_loss: 0.9331 - val_accuracy: 0.4111\n",
      "Epoch 4/100\n",
      "7200/7200 [==============================] - 4s 523us/step - loss: 0.5604 - accuracy: 0.7053 - val_loss: 0.7501 - val_accuracy: 0.5311\n",
      "Epoch 5/100\n",
      "7200/7200 [==============================] - 4s 527us/step - loss: 0.5465 - accuracy: 0.7165 - val_loss: 0.8982 - val_accuracy: 0.4433\n",
      "Epoch 6/100\n",
      "7200/7200 [==============================] - 4s 526us/step - loss: 0.5378 - accuracy: 0.7224 - val_loss: 0.7890 - val_accuracy: 0.5450\n",
      "Epoch 7/100\n",
      "7200/7200 [==============================] - 4s 528us/step - loss: 0.5207 - accuracy: 0.7328 - val_loss: 0.9490 - val_accuracy: 0.4694\n",
      "Epoch 8/100\n",
      "7200/7200 [==============================] - 4s 528us/step - loss: 0.5088 - accuracy: 0.7432 - val_loss: 1.0465 - val_accuracy: 0.4589\n",
      "Epoch 9/100\n",
      "7200/7200 [==============================] - 4s 526us/step - loss: 0.4921 - accuracy: 0.7522 - val_loss: 0.8745 - val_accuracy: 0.5194\n",
      "Epoch 10/100\n",
      "7200/7200 [==============================] - 4s 521us/step - loss: 0.4784 - accuracy: 0.7592 - val_loss: 1.1058 - val_accuracy: 0.4361\n",
      "Epoch 11/100\n",
      "7200/7200 [==============================] - 4s 587us/step - loss: 0.4693 - accuracy: 0.7650 - val_loss: 0.7082 - val_accuracy: 0.6383\n",
      "Epoch 12/100\n",
      "7200/7200 [==============================] - 4s 548us/step - loss: 0.4602 - accuracy: 0.7725 - val_loss: 0.9268 - val_accuracy: 0.5083\n",
      "Epoch 13/100\n",
      "7200/7200 [==============================] - 4s 544us/step - loss: 0.4412 - accuracy: 0.7832 - val_loss: 0.9113 - val_accuracy: 0.5156\n",
      "Epoch 14/100\n",
      "7200/7200 [==============================] - 4s 543us/step - loss: 0.4413 - accuracy: 0.7847 - val_loss: 1.0863 - val_accuracy: 0.4922\n",
      "Epoch 15/100\n",
      "7200/7200 [==============================] - 4s 557us/step - loss: 0.4264 - accuracy: 0.7911 - val_loss: 1.1582 - val_accuracy: 0.4717\n",
      "Epoch 16/100\n",
      "7200/7200 [==============================] - 4s 563us/step - loss: 0.4179 - accuracy: 0.8007 - val_loss: 0.9681 - val_accuracy: 0.5294\n",
      "Epoch 17/100\n",
      "7200/7200 [==============================] - 4s 579us/step - loss: 0.4092 - accuracy: 0.7985 - val_loss: 1.0631 - val_accuracy: 0.5467\n",
      "Epoch 18/100\n",
      "7200/7200 [==============================] - 4s 552us/step - loss: 0.3936 - accuracy: 0.8128 - val_loss: 1.0747 - val_accuracy: 0.5556\n",
      "Epoch 19/100\n",
      "7200/7200 [==============================] - 4s 534us/step - loss: 0.3822 - accuracy: 0.8144 - val_loss: 0.9943 - val_accuracy: 0.5772\n",
      "Epoch 20/100\n",
      "7200/7200 [==============================] - 4s 555us/step - loss: 0.3804 - accuracy: 0.8149 - val_loss: 1.3740 - val_accuracy: 0.4617\n",
      "Epoch 21/100\n",
      "7200/7200 [==============================] - 4s 538us/step - loss: 0.3722 - accuracy: 0.8226 - val_loss: 1.3773 - val_accuracy: 0.4528\n",
      "Epoch 22/100\n",
      "7200/7200 [==============================] - 4s 529us/step - loss: 0.3679 - accuracy: 0.8275 - val_loss: 1.5496 - val_accuracy: 0.4056\n",
      "Epoch 23/100\n",
      "7200/7200 [==============================] - 4s 529us/step - loss: 0.3583 - accuracy: 0.8307 - val_loss: 1.2090 - val_accuracy: 0.4928\n",
      "Epoch 24/100\n",
      "7200/7200 [==============================] - 4s 530us/step - loss: 0.3548 - accuracy: 0.8310 - val_loss: 1.2230 - val_accuracy: 0.5144\n",
      "Epoch 25/100\n",
      "7200/7200 [==============================] - 4s 530us/step - loss: 0.3488 - accuracy: 0.8321 - val_loss: 1.3876 - val_accuracy: 0.5450\n",
      "Epoch 26/100\n",
      "7200/7200 [==============================] - 4s 531us/step - loss: 0.3343 - accuracy: 0.8421 - val_loss: 1.2747 - val_accuracy: 0.5311\n",
      "Epoch 27/100\n",
      "7200/7200 [==============================] - 4s 566us/step - loss: 0.3382 - accuracy: 0.8428 - val_loss: 1.2733 - val_accuracy: 0.4994\n",
      "Epoch 28/100\n",
      "7200/7200 [==============================] - 4s 533us/step - loss: 0.3344 - accuracy: 0.8403 - val_loss: 1.4394 - val_accuracy: 0.4867\n",
      "Epoch 29/100\n",
      "7200/7200 [==============================] - 4s 529us/step - loss: 0.3185 - accuracy: 0.8494 - val_loss: 1.8697 - val_accuracy: 0.4494\n",
      "Epoch 30/100\n",
      "7200/7200 [==============================] - 4s 532us/step - loss: 0.3168 - accuracy: 0.8521 - val_loss: 1.7539 - val_accuracy: 0.4539\n",
      "Epoch 31/100\n",
      "7200/7200 [==============================] - 4s 525us/step - loss: 0.3149 - accuracy: 0.8529 - val_loss: 1.6257 - val_accuracy: 0.5083\n",
      "Epoch 32/100\n",
      "7200/7200 [==============================] - 4s 529us/step - loss: 0.3118 - accuracy: 0.8499 - val_loss: 1.5528 - val_accuracy: 0.5039\n",
      "Epoch 33/100\n",
      "7200/7200 [==============================] - 4s 529us/step - loss: 0.3103 - accuracy: 0.8525 - val_loss: 1.8359 - val_accuracy: 0.4611\n",
      "Epoch 34/100\n",
      "7200/7200 [==============================] - 4s 529us/step - loss: 0.3036 - accuracy: 0.8592 - val_loss: 1.4548 - val_accuracy: 0.5100\n",
      "Epoch 35/100\n",
      "7200/7200 [==============================] - 4s 528us/step - loss: 0.3013 - accuracy: 0.8626 - val_loss: 1.7734 - val_accuracy: 0.4733\n",
      "Epoch 36/100\n",
      "7200/7200 [==============================] - 4s 526us/step - loss: 0.3013 - accuracy: 0.8639 - val_loss: 1.6138 - val_accuracy: 0.5244\n",
      "Epoch 37/100\n",
      "7200/7200 [==============================] - 4s 527us/step - loss: 0.2926 - accuracy: 0.8653 - val_loss: 1.7523 - val_accuracy: 0.4594\n",
      "Epoch 38/100\n",
      "7200/7200 [==============================] - 4s 535us/step - loss: 0.2846 - accuracy: 0.8682 - val_loss: 1.7183 - val_accuracy: 0.5222\n",
      "Epoch 39/100\n",
      "7200/7200 [==============================] - 4s 528us/step - loss: 0.2874 - accuracy: 0.8656 - val_loss: 1.8124 - val_accuracy: 0.5144\n",
      "Epoch 40/100\n",
      "7200/7200 [==============================] - 4s 529us/step - loss: 0.2856 - accuracy: 0.8662 - val_loss: 1.9478 - val_accuracy: 0.4733\n",
      "Epoch 41/100\n",
      "7200/7200 [==============================] - 4s 531us/step - loss: 0.2845 - accuracy: 0.8679 - val_loss: 1.9428 - val_accuracy: 0.4589\n",
      "Epoch 42/100\n",
      "7200/7200 [==============================] - 4s 617us/step - loss: 0.2719 - accuracy: 0.8715 - val_loss: 2.2061 - val_accuracy: 0.4217\n",
      "Epoch 43/100\n",
      "7200/7200 [==============================] - 4s 615us/step - loss: 0.2762 - accuracy: 0.8757 - val_loss: 1.6647 - val_accuracy: 0.5100\n",
      "Epoch 44/100\n",
      "7200/7200 [==============================] - 4s 577us/step - loss: 0.2650 - accuracy: 0.8815 - val_loss: 1.8960 - val_accuracy: 0.4533\n",
      "Epoch 45/100\n",
      "7200/7200 [==============================] - 4s 552us/step - loss: 0.2667 - accuracy: 0.8797 - val_loss: 1.8991 - val_accuracy: 0.4989\n",
      "Epoch 46/100\n",
      "7200/7200 [==============================] - 4s 531us/step - loss: 0.2621 - accuracy: 0.8810 - val_loss: 1.9217 - val_accuracy: 0.4911\n",
      "Epoch 47/100\n",
      "7200/7200 [==============================] - 4s 531us/step - loss: 0.2530 - accuracy: 0.8840 - val_loss: 1.7606 - val_accuracy: 0.5389\n",
      "Epoch 48/100\n",
      "7200/7200 [==============================] - 4s 539us/step - loss: 0.2602 - accuracy: 0.8783 - val_loss: 2.0684 - val_accuracy: 0.4867\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200/7200 [==============================] - 4s 524us/step - loss: 0.2550 - accuracy: 0.8831 - val_loss: 2.0600 - val_accuracy: 0.4556\n",
      "Epoch 50/100\n",
      "7200/7200 [==============================] - 4s 522us/step - loss: 0.2505 - accuracy: 0.8844 - val_loss: 2.4255 - val_accuracy: 0.4439\n",
      "Epoch 51/100\n",
      "7200/7200 [==============================] - 4s 527us/step - loss: 0.2493 - accuracy: 0.8849 - val_loss: 2.1931 - val_accuracy: 0.4956\n",
      "Epoch 52/100\n",
      "7200/7200 [==============================] - 4s 541us/step - loss: 0.2487 - accuracy: 0.8908 - val_loss: 1.8482 - val_accuracy: 0.5000\n",
      "Epoch 53/100\n",
      "7200/7200 [==============================] - 4s 533us/step - loss: 0.2473 - accuracy: 0.8836 - val_loss: 2.3952 - val_accuracy: 0.4744\n",
      "Epoch 54/100\n",
      "7200/7200 [==============================] - 4s 530us/step - loss: 0.2358 - accuracy: 0.8961 - val_loss: 1.9956 - val_accuracy: 0.4739\n",
      "Epoch 55/100\n",
      "7200/7200 [==============================] - 4s 539us/step - loss: 0.2407 - accuracy: 0.8914 - val_loss: 2.1286 - val_accuracy: 0.4939\n",
      "Epoch 56/100\n",
      "7200/7200 [==============================] - 5s 643us/step - loss: 0.2452 - accuracy: 0.8906 - val_loss: 2.0748 - val_accuracy: 0.5178\n",
      "Epoch 57/100\n",
      "7200/7200 [==============================] - 5s 647us/step - loss: 0.2341 - accuracy: 0.8936 - val_loss: 2.1572 - val_accuracy: 0.4544\n",
      "Epoch 58/100\n",
      "7200/7200 [==============================] - 5s 691us/step - loss: 0.2431 - accuracy: 0.8914 - val_loss: 2.0959 - val_accuracy: 0.4928\n",
      "Epoch 59/100\n",
      "7200/7200 [==============================] - 4s 578us/step - loss: 0.2322 - accuracy: 0.8940 - val_loss: 1.7571 - val_accuracy: 0.5411\n",
      "Epoch 60/100\n",
      "7200/7200 [==============================] - 4s 545us/step - loss: 0.2399 - accuracy: 0.8940 - val_loss: 2.2534 - val_accuracy: 0.4828\n",
      "Epoch 61/100\n",
      "7200/7200 [==============================] - 4s 542us/step - loss: 0.2223 - accuracy: 0.8990 - val_loss: 2.9559 - val_accuracy: 0.4222\n",
      "Epoch 62/100\n",
      "7200/7200 [==============================] - 4s 544us/step - loss: 0.2304 - accuracy: 0.8990 - val_loss: 1.9819 - val_accuracy: 0.5306\n",
      "Epoch 63/100\n",
      "7200/7200 [==============================] - 4s 547us/step - loss: 0.2285 - accuracy: 0.8975 - val_loss: 2.7220 - val_accuracy: 0.4611\n",
      "Epoch 64/100\n",
      "7200/7200 [==============================] - 4s 543us/step - loss: 0.2139 - accuracy: 0.9019 - val_loss: 2.6149 - val_accuracy: 0.5000\n",
      "Epoch 65/100\n",
      "7200/7200 [==============================] - 4s 595us/step - loss: 0.2189 - accuracy: 0.9033 - val_loss: 2.5236 - val_accuracy: 0.4928\n",
      "Epoch 66/100\n",
      "7200/7200 [==============================] - 4s 564us/step - loss: 0.2239 - accuracy: 0.9028 - val_loss: 2.4388 - val_accuracy: 0.4900\n",
      "Epoch 67/100\n",
      "7200/7200 [==============================] - 4s 547us/step - loss: 0.2172 - accuracy: 0.9043 - val_loss: 2.8143 - val_accuracy: 0.4778\n",
      "Epoch 68/100\n",
      "7200/7200 [==============================] - 4s 554us/step - loss: 0.2278 - accuracy: 0.9026 - val_loss: 2.5547 - val_accuracy: 0.4933\n",
      "Epoch 69/100\n",
      "7200/7200 [==============================] - 4s 547us/step - loss: 0.2184 - accuracy: 0.9018 - val_loss: 2.6409 - val_accuracy: 0.4861\n",
      "Epoch 70/100\n",
      "7200/7200 [==============================] - 4s 539us/step - loss: 0.2136 - accuracy: 0.9076 - val_loss: 2.7211 - val_accuracy: 0.4650\n",
      "Epoch 71/100\n",
      "7200/7200 [==============================] - 4s 542us/step - loss: 0.2145 - accuracy: 0.9033 - val_loss: 2.4315 - val_accuracy: 0.5094\n",
      "Epoch 72/100\n",
      "7200/7200 [==============================] - 4s 540us/step - loss: 0.2146 - accuracy: 0.9065 - val_loss: 2.5415 - val_accuracy: 0.5100\n",
      "Epoch 73/100\n",
      "7200/7200 [==============================] - 4s 545us/step - loss: 0.2150 - accuracy: 0.9018 - val_loss: 2.5069 - val_accuracy: 0.5011\n",
      "Epoch 74/100\n",
      "7200/7200 [==============================] - 4s 542us/step - loss: 0.2086 - accuracy: 0.9071 - val_loss: 2.7514 - val_accuracy: 0.4833\n",
      "Epoch 75/100\n",
      "7200/7200 [==============================] - 4s 543us/step - loss: 0.2087 - accuracy: 0.9065 - val_loss: 2.8468 - val_accuracy: 0.4761\n",
      "Epoch 76/100\n",
      "7200/7200 [==============================] - 4s 543us/step - loss: 0.2163 - accuracy: 0.9024 - val_loss: 2.5315 - val_accuracy: 0.4911\n",
      "Epoch 77/100\n",
      "7200/7200 [==============================] - 4s 542us/step - loss: 0.2058 - accuracy: 0.9051 - val_loss: 3.3054 - val_accuracy: 0.4511\n",
      "Epoch 78/100\n",
      "7200/7200 [==============================] - 4s 544us/step - loss: 0.2042 - accuracy: 0.9072 - val_loss: 2.6979 - val_accuracy: 0.4733\n",
      "Epoch 79/100\n",
      "7200/7200 [==============================] - 4s 546us/step - loss: 0.2035 - accuracy: 0.9128 - val_loss: 2.4216 - val_accuracy: 0.5161\n",
      "Epoch 80/100\n",
      "7200/7200 [==============================] - 4s 555us/step - loss: 0.2007 - accuracy: 0.9099 - val_loss: 2.5956 - val_accuracy: 0.5106\n",
      "Epoch 81/100\n",
      "7200/7200 [==============================] - 4s 547us/step - loss: 0.2127 - accuracy: 0.9044 - val_loss: 2.3531 - val_accuracy: 0.5011\n",
      "Epoch 82/100\n",
      "7200/7200 [==============================] - 4s 542us/step - loss: 0.1979 - accuracy: 0.9146 - val_loss: 2.6005 - val_accuracy: 0.4850\n",
      "Epoch 83/100\n",
      "7200/7200 [==============================] - 4s 549us/step - loss: 0.2034 - accuracy: 0.9101 - val_loss: 2.5745 - val_accuracy: 0.4783\n",
      "Epoch 84/100\n",
      "7200/7200 [==============================] - 4s 545us/step - loss: 0.2024 - accuracy: 0.9115 - val_loss: 2.5876 - val_accuracy: 0.4783\n",
      "Epoch 85/100\n",
      "7200/7200 [==============================] - 4s 548us/step - loss: 0.1974 - accuracy: 0.9087 - val_loss: 2.8719 - val_accuracy: 0.4706\n",
      "Epoch 86/100\n",
      "7200/7200 [==============================] - 4s 548us/step - loss: 0.2099 - accuracy: 0.9100 - val_loss: 2.6359 - val_accuracy: 0.4733\n",
      "Epoch 87/100\n",
      "7200/7200 [==============================] - 4s 546us/step - loss: 0.1992 - accuracy: 0.9110 - val_loss: 2.4947 - val_accuracy: 0.5150\n",
      "Epoch 88/100\n",
      "7200/7200 [==============================] - 4s 546us/step - loss: 0.1897 - accuracy: 0.9142 - val_loss: 2.8628 - val_accuracy: 0.4967\n",
      "Epoch 89/100\n",
      "7200/7200 [==============================] - 4s 549us/step - loss: 0.2061 - accuracy: 0.9119 - val_loss: 2.8144 - val_accuracy: 0.4828\n",
      "Epoch 90/100\n",
      "7200/7200 [==============================] - 4s 545us/step - loss: 0.2020 - accuracy: 0.9153 - val_loss: 2.4658 - val_accuracy: 0.5011\n",
      "Epoch 91/100\n",
      "7200/7200 [==============================] - 4s 545us/step - loss: 0.1946 - accuracy: 0.9142 - val_loss: 2.4818 - val_accuracy: 0.5067\n",
      "Epoch 92/100\n",
      "7200/7200 [==============================] - 4s 582us/step - loss: 0.1854 - accuracy: 0.9176 - val_loss: 2.2991 - val_accuracy: 0.5217\n",
      "Epoch 93/100\n",
      "7200/7200 [==============================] - 4s 548us/step - loss: 0.1912 - accuracy: 0.9157 - val_loss: 2.4261 - val_accuracy: 0.5089\n",
      "Epoch 94/100\n",
      "7200/7200 [==============================] - 4s 552us/step - loss: 0.1941 - accuracy: 0.9179 - val_loss: 2.5544 - val_accuracy: 0.4872\n",
      "Epoch 95/100\n",
      "7200/7200 [==============================] - 4s 548us/step - loss: 0.1938 - accuracy: 0.9175 - val_loss: 2.5158 - val_accuracy: 0.5011\n",
      "Epoch 96/100\n",
      "7200/7200 [==============================] - 4s 565us/step - loss: 0.1786 - accuracy: 0.9222 - val_loss: 3.0517 - val_accuracy: 0.5100\n",
      "Epoch 97/100\n",
      "7200/7200 [==============================] - 4s 586us/step - loss: 0.1973 - accuracy: 0.9165 - val_loss: 2.6387 - val_accuracy: 0.4911\n",
      "Epoch 98/100\n",
      "7200/7200 [==============================] - 4s 565us/step - loss: 0.1865 - accuracy: 0.9225 - val_loss: 2.5944 - val_accuracy: 0.4928\n",
      "Epoch 99/100\n",
      "7200/7200 [==============================] - 4s 557us/step - loss: 0.1922 - accuracy: 0.9143 - val_loss: 2.5282 - val_accuracy: 0.5089\n",
      "Epoch 100/100\n",
      "7200/7200 [==============================] - 4s 546us/step - loss: 0.1866 - accuracy: 0.9206 - val_loss: 2.6282 - val_accuracy: 0.5106\n",
      "saved model!\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(100, activation='relu',input_shape=(train_x.shape[1],)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "filepath=\"sequencing_the_data_try_n_error.{epoch:02d}-{val_loss:.4f}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "csv_logger = CSVLogger('final_log.csv', append=True, separator=';')\n",
    "\n",
    "model.fit(train_x, train_y,\n",
    "    batch_size=5,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    validation_split=0.20,\n",
    "    shuffle=True,callbacks = [csv_logger])\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open('model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights('model.h5')\n",
    "\n",
    "print('saved model!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'text', 'HS', 'TR', 'AG']\n"
     ]
    }
   ],
   "source": [
    "#test.tsv\n",
    "test=pd.read_table('dev_en.tsv',delimiter='\\t',encoding='utf-8')\n",
    "print(list(test.columns.values)) #file header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>HS</th>\n",
       "      <th>TR</th>\n",
       "      <th>AG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18201</td>\n",
       "      <td>I swear I’m getting to places just in the nick...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18202</td>\n",
       "      <td>I’m an immigrant — and Trump is right on immig...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18203</td>\n",
       "      <td>#IllegalImmigrants #IllegalAliens #ElectoralSy...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18204</td>\n",
       "      <td>@DRUDGE_REPORT We have our own invasion issues...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18205</td>\n",
       "      <td>Worker Charged With Sexually Molesting Eight C...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text  HS  TR  AG\n",
       "0  18201  I swear I’m getting to places just in the nick...   0   0   0\n",
       "1  18202  I’m an immigrant — and Trump is right on immig...   0   0   0\n",
       "2  18203  #IllegalImmigrants #IllegalAliens #ElectoralSy...   1   0   1\n",
       "3  18204  @DRUDGE_REPORT We have our own invasion issues...   1   0   1\n",
       "4  18205  Worker Charged With Sexually Molesting Eight C...   0   0   0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('segmentation_test.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for row in test.text:\n",
    "        row = re.sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\",\"url\",row)\n",
    "        row = \" \".join([word for word in row.split() if word not in my_word_stop])\n",
    "        row = re.sub(r\"(.)\\1+\", r\"\\1\",row)\n",
    "        rest_array = [text.encode(\"utf8\") for text in row]\n",
    "        rest_array = b\"\".join(rest_array)\n",
    "        writer.writerow(rest_array)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "test_final=pd.read_csv('segmentation_test.csv',delimiter='\\t',header=None)\n",
    "print(list(test_final.columns.values)) #file header\n",
    "test_final[0] = test_final[0].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_array[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "word not found :  0\n",
      "1000\n",
      "word found :  [36, 2, 3, 7, 10, 17, 1, 2, 24, 2, 7, 17, 1, 19, 8, 14, 3, 6, 1, 26, 1, 22, 6, 8, 19, 1, 12, 3, 9, 1, 28, 14, 2, 25, 8, 1, 4, 12, 2, 5, 7, 1, 21, 2, 13, 1, 52, 9, 2, 53, 11, 3, 10, 1, 12, 3, 7, 2, 9, 14, 2, 6, 4, 31, 3, 20, 11, 9, 2, 31, 7, 3, 18, 2, 1, 3, 10, 1, 52, 1, 17, 2, 9, 1, 5, 4, 32, 9, 1, 12, 2, 3, 7, 4, 20, 7, 2, 3, 22, 5, 6, 16, 31, 1, 17, 2, 9, 1, 4, 12, 2, 7, 2, 32, 9, 1, 14, 11, 15, 12, 1, 62, 1, 13, 8, 23]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=train_x.shape[1])\n",
    "\n",
    "\n",
    "with open('dictionary.p', 'rb') as fp:\n",
    "    dictionary = pickle.load(fp)\n",
    "    \n",
    "    \n",
    "not_found_word_list = []\n",
    "def convert_text_to_index_array(text):\n",
    "    words = kpt.text_to_word_sequence(text,filters='',lower=False,split=',')\n",
    "    wordIndices = []\n",
    "    no_word = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            wordIndices.append(dictionary[word])\n",
    "        else:\n",
    "            if word == \"\":\n",
    "                not_found_word_list.append(word)\n",
    "                no_word = no_word + 1\n",
    "    \n",
    "    return wordIndices,no_word\n",
    "\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "model.load_weights('model.h5')\n",
    "with open('fileName.csv', 'w') as f:\n",
    "    count=0\n",
    "    no_words = 0\n",
    "    for row in test_final[0]:\n",
    "        evalSentence = row\n",
    "        testArr,no_word = convert_text_to_index_array(evalSentence)\n",
    "        input = tokenizer.sequences_to_matrix([testArr], mode='binary')\n",
    "        pred = model.predict(input)\n",
    "        f.write(str(np.argmax(pred)) + \"\\n\")\n",
    "        count+=1\n",
    "        no_words+=no_word\n",
    "f.close()\n",
    "print(count)\n",
    "print(\"word not found : \", no_words)\n",
    "print(count)\n",
    "print(\"word found : \", wordIndices)\n",
    "with open('not_found_word_list.csv', 'w') as f:\n",
    "    for word in not_found_word_list:\n",
    "        f.write(str(word)+\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
