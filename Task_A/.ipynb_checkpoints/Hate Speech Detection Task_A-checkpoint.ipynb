{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Based Fusion Approach for Hate Speech Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import copy\n",
    "import re\n",
    "import csv\n",
    "from keras.utils import to_categorical\n",
    "import codecs\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation , LSTM , Input , Embedding\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Input, Dense, concatenate, Activation, Average\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout , Bidirectional\n",
    "from keras.layers import Flatten , LSTM , Reshape, LeakyReLU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
    "from keras.regularizers import L1L2\n",
    "from keras import optimizers\n",
    "from keras.callbacks import CSVLogger\n",
    "import keras\n",
    "import keras.preprocessing.text as kpt\n",
    "#from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\M Srikant\\\\Desktop\\\\160118862036_DL_HPD\\\\CODE\\\\Original Code\\\\Task_A\\\\Data'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"C:/Users/M Srikant/Desktop/160118862036_DL_HPD/CODE/Original Code/Task_A/Data\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'text', 'HS', 'TR', 'AG']\n"
     ]
    }
   ],
   "source": [
    "#train.tsv\n",
    "train=pd.read_table('train_en.tsv',delimiter='\\t',encoding='utf-8')\n",
    "print(list(train.columns.values)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>HS</th>\n",
       "      <th>TR</th>\n",
       "      <th>AG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201</td>\n",
       "      <td>Hurray, saving us $$$ in so many ways @potus @...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202</td>\n",
       "      <td>Why would young fighting age men be the vast m...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>203</td>\n",
       "      <td>@KamalaHarris Illegals Dump their Kids at the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>204</td>\n",
       "      <td>NY Times: 'Nearly All White' States Pose 'an A...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>205</td>\n",
       "      <td>Orban in Brussels: European leaders are ignori...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                               text  HS  TR  AG\n",
       "0  201  Hurray, saving us $$$ in so many ways @potus @...   1   0   0\n",
       "1  202  Why would young fighting age men be the vast m...   1   0   0\n",
       "2  203  @KamalaHarris Illegals Dump their Kids at the ...   1   0   0\n",
       "3  204  NY Times: 'Nearly All White' States Pose 'an A...   0   0   0\n",
       "4  205  Orban in Brussels: European leaders are ignori...   0   0   0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id      9000\n",
       "text    9000\n",
       "HS      9000\n",
       "TR      9000\n",
       "AG      9000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Hurray, saving us $$$ in so many ways @potus @...\n",
      "1    Why would young fighting age men be the vast m...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(my_df.text.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_word_stop = ['the','in','of','is','a','to','an','be','are','for','was','it','as','on', 'so', 'at']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('segmentation_train_dev.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for row in my_df.text:\n",
    "        row = re.sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\",\"url\",row)\n",
    "        row = \" \".join([word for word in row.split() if word not in my_word_stop])\n",
    "        row = re.sub(r\"(.)\\1+\", r\"\\1\",row)\n",
    "        rest_array = [text.encode(\"utf8\") for text in row]\n",
    "        rest_array = b\"\".join(rest_array)\n",
    "        writer.writerow(rest_array)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_table('segmentation_train_dev.csv',delimiter='\\t',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    9000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[0] = train[0].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = my_df.HS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df.HS.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = Tokenizer(lower=False,filters=',',char_level=False)\n",
    "tokenizer.fit_on_texts(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M Srikant\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dictionary = tokenizer.word_index\n",
    "with open('dictionary.p', 'wb') as fp:\n",
    "    pickle.dump(dictionary, fp)\n",
    "\n",
    "\n",
    "def convert_text_to_index_array(text):\n",
    "    temp_wordIndices = []\n",
    "    for word in kpt.text_to_word_sequence(text,filters=',',lower=False):\n",
    "        if word in dictionary:\n",
    "            temp_wordIndices.append(dictionary[word])\n",
    "    return temp_wordIndices\n",
    "\n",
    "allWordIndices = []\n",
    "for text in train_x:\n",
    "    wordIndices = convert_text_to_index_array(text)\n",
    "    allWordIndices.append(wordIndices)\n",
    "\n",
    "allWordIndices = np.asarray(allWordIndices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 170 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "print('Found %s unique tokens.' % len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = tokenizer.sequences_to_matrix(allWordIndices, mode='binary')\n",
    "train_y = to_categorical(train_y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.advanced_activations import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               17200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 37,802\n",
      "Trainable params: 37,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 7200 samples, validate on 1800 samples\n",
      "Epoch 1/15\n",
      "7200/7200 [==============================] - 10s 1ms/step - loss: 0.6420 - accuracy: 0.6222 - val_loss: 0.6725 - val_accuracy: 0.5811\n",
      "Epoch 2/15\n",
      "7200/7200 [==============================] - 4s 585us/step - loss: 0.5934 - accuracy: 0.6796 - val_loss: 0.8389 - val_accuracy: 0.4567\n",
      "Epoch 3/15\n",
      "7200/7200 [==============================] - 4s 611us/step - loss: 0.5713 - accuracy: 0.6943 - val_loss: 0.8551 - val_accuracy: 0.4689\n",
      "Epoch 4/15\n",
      "7200/7200 [==============================] - 4s 584us/step - loss: 0.5555 - accuracy: 0.7044 - val_loss: 0.8626 - val_accuracy: 0.4550\n",
      "Epoch 5/15\n",
      "7200/7200 [==============================] - 4s 599us/step - loss: 0.5417 - accuracy: 0.7240 - val_loss: 0.7597 - val_accuracy: 0.5589\n",
      "Epoch 6/15\n",
      "7200/7200 [==============================] - 5s 656us/step - loss: 0.5297 - accuracy: 0.7244 - val_loss: 0.8818 - val_accuracy: 0.4672\n",
      "Epoch 7/15\n",
      "7200/7200 [==============================] - 5s 679us/step - loss: 0.5178 - accuracy: 0.7315 - val_loss: 0.8247 - val_accuracy: 0.5083\n",
      "Epoch 8/15\n",
      "7200/7200 [==============================] - 5s 649us/step - loss: 0.5007 - accuracy: 0.7494 - val_loss: 1.0511 - val_accuracy: 0.4333\n",
      "Epoch 9/15\n",
      "7200/7200 [==============================] - 5s 674us/step - loss: 0.4909 - accuracy: 0.7515 - val_loss: 0.6877 - val_accuracy: 0.6439\n",
      "Epoch 10/15\n",
      "7200/7200 [==============================] - 5s 635us/step - loss: 0.4767 - accuracy: 0.7558 - val_loss: 1.0711 - val_accuracy: 0.4156\n",
      "Epoch 11/15\n",
      "7200/7200 [==============================] - 4s 586us/step - loss: 0.4627 - accuracy: 0.7657 - val_loss: 1.0980 - val_accuracy: 0.4567\n",
      "Epoch 12/15\n",
      "7200/7200 [==============================] - 5s 656us/step - loss: 0.4547 - accuracy: 0.7719 - val_loss: 1.0526 - val_accuracy: 0.4967\n",
      "Epoch 13/15\n",
      "7200/7200 [==============================] - 4s 612us/step - loss: 0.4443 - accuracy: 0.7779 - val_loss: 1.1482 - val_accuracy: 0.4772\n",
      "Epoch 14/15\n",
      "7200/7200 [==============================] - 4s 612us/step - loss: 0.4320 - accuracy: 0.7832 - val_loss: 1.1474 - val_accuracy: 0.4567\n",
      "Epoch 15/15\n",
      "7200/7200 [==============================] - 4s 596us/step - loss: 0.4222 - accuracy: 0.7924 - val_loss: 1.1505 - val_accuracy: 0.4800\n",
      "saved model!\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(100, activation='relu',input_shape=(train_x.shape[1],)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "filepath=\"sequencing_the_data_try_n_error.{epoch:02d}-{val_loss:.4f}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "csv_logger = CSVLogger('final_log.csv', append=True, separator=';')\n",
    "\n",
    "model.fit(train_x, train_y,\n",
    "    batch_size=5,\n",
    "    epochs=15,\n",
    "    verbose=1,\n",
    "    validation_split=0.20,\n",
    "    shuffle=True,callbacks = [csv_logger])\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open('model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights('model.h5')\n",
    "\n",
    "print('saved model!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'text', 'HS', 'TR', 'AG']\n"
     ]
    }
   ],
   "source": [
    "#test.tsv\n",
    "test=pd.read_table('dev_en.tsv',delimiter='\\t',encoding='utf-8')\n",
    "print(list(test.columns.values)) #file header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>HS</th>\n",
       "      <th>TR</th>\n",
       "      <th>AG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18201</td>\n",
       "      <td>I swear I’m getting to places just in the nick...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18202</td>\n",
       "      <td>I’m an immigrant — and Trump is right on immig...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18203</td>\n",
       "      <td>#IllegalImmigrants #IllegalAliens #ElectoralSy...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18204</td>\n",
       "      <td>@DRUDGE_REPORT We have our own invasion issues...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18205</td>\n",
       "      <td>Worker Charged With Sexually Molesting Eight C...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text  HS  TR  AG\n",
       "0  18201  I swear I’m getting to places just in the nick...   0   0   0\n",
       "1  18202  I’m an immigrant — and Trump is right on immig...   0   0   0\n",
       "2  18203  #IllegalImmigrants #IllegalAliens #ElectoralSy...   1   0   1\n",
       "3  18204  @DRUDGE_REPORT We have our own invasion issues...   1   0   1\n",
       "4  18205  Worker Charged With Sexually Molesting Eight C...   0   0   0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('segmentation_test.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for row in test.text:\n",
    "        row = re.sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\",\"url\",row)\n",
    "        row = \" \".join([word for word in row.split() if word not in my_word_stop])\n",
    "        row = re.sub(r\"(.)\\1+\", r\"\\1\",row)\n",
    "        rest_array = [text.encode(\"utf8\") for text in row]\n",
    "        rest_array = b\"\".join(rest_array)\n",
    "        writer.writerow(rest_array)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "test_final=pd.read_csv('segmentation_test.csv',delimiter='\\t',header=None)\n",
    "print(list(test_final.columns.values)) #file header\n",
    "test_final[0] = test_final[0].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_array[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "word not found :  0\n",
      "1000\n",
      "word found :  [36, 2, 3, 7, 10, 17, 1, 2, 24, 2, 7, 17, 1, 19, 8, 14, 3, 6, 1, 26, 1, 22, 6, 8, 19, 1, 12, 3, 9, 1, 28, 14, 2, 25, 8, 1, 4, 12, 2, 5, 7, 1, 21, 2, 13, 1, 52, 9, 2, 53, 11, 3, 10, 1, 12, 3, 7, 2, 9, 14, 2, 6, 4, 31, 3, 20, 11, 9, 2, 31, 7, 3, 18, 2, 1, 3, 10, 1, 52, 1, 17, 2, 9, 1, 5, 4, 32, 9, 1, 12, 2, 3, 7, 4, 20, 7, 2, 3, 22, 5, 6, 16, 31, 1, 17, 2, 9, 1, 4, 12, 2, 7, 2, 32, 9, 1, 14, 11, 15, 12, 1, 62, 1, 13, 8, 23]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=train_x.shape[1])\n",
    "\n",
    "\n",
    "with open('dictionary.p', 'rb') as fp:\n",
    "    dictionary = pickle.load(fp)\n",
    "    \n",
    "    \n",
    "not_found_word_list = []\n",
    "def convert_text_to_index_array(text):\n",
    "    words = kpt.text_to_word_sequence(text,filters='',lower=False,split=',')\n",
    "    wordIndices = []\n",
    "    no_word = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            wordIndices.append(dictionary[word])\n",
    "        else:\n",
    "            if word == \"\":\n",
    "                not_found_word_list.append(word)\n",
    "                no_word = no_word + 1\n",
    "    \n",
    "    return wordIndices,no_word\n",
    "\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "model.load_weights('model.h5')\n",
    "with open('fileName.csv', 'w') as f:\n",
    "    count=0\n",
    "    no_words = 0\n",
    "    for row in test_final[0]:\n",
    "        evalSentence = row\n",
    "        testArr,no_word = convert_text_to_index_array(evalSentence)\n",
    "        input = tokenizer.sequences_to_matrix([testArr], mode='binary')\n",
    "        pred = model.predict(input)\n",
    "        f.write(str(np.argmax(pred)) + \"\\n\")\n",
    "        count+=1\n",
    "        no_words+=no_word\n",
    "f.close()\n",
    "print(count)\n",
    "print(\"word not found : \", no_words)\n",
    "print(count)\n",
    "print(\"word found : \", wordIndices)\n",
    "with open('not_found_word_list.csv', 'w') as f:\n",
    "    for word in not_found_word_list:\n",
    "        f.write(str(word)+\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
